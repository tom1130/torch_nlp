{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcd03bfcb10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/share/data/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('/root/share/data/train_label.pkl', 'rb') as f:\n",
    "    train_label = pickle.load(f)\n",
    "with open('/root/share/data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open('/root/share/data/test_data.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = random_split([[train_data[i], train_label[i]] for i in range(len(train_label))], [len(train_label)-2000,2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, target_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embeds = self.word_embeddings(input)\n",
    "        _, output = self.lstm(embeds)\n",
    "        output = self.dropout(output[0])\n",
    "        output = self.linear(output)\n",
    "        output = F.log_softmax(output, dim=1).view(input.size()[0], -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 100\n",
    "VOCAB_SIZE = 42000\n",
    "TARGET_SIZE = 3\n",
    "\n",
    "# train hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch loss: 0.789525, accuracy : 86.97%\n",
      "Valid Accuracy : 84.53%\n",
      "2 epoch loss: 0.786458, accuracy : 87.77%\n",
      "Valid Accuracy : 85.76%\n",
      "3 epoch loss: 0.785891, accuracy : 87.92%\n",
      "Valid Accuracy : 84.73%\n"
     ]
    }
   ],
   "source": [
    "batch_num = len(train_dataloader)\n",
    "best_accuracy = 0.0\n",
    "trigger_times = 0\n",
    "patience = 5\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(train_dataloader, 1):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'{epoch + 1} epoch loss: {running_loss / 297:.6f}, accuracy : {100*correct/len(train):.2f}%')\n",
    "    running_loss = 0.0\n",
    "\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        valid_total += labels.size(0)\n",
    "        valid_correct += (predicted == labels).sum().item()\n",
    "    print(f'Valid Accuracy : {100*valid_correct/valid_total:.2f}%')\n",
    "    # early stopping\n",
    "    current_accuracy = valid_correct/valid_total\n",
    "    if current_accuracy < best_accuracy:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience :\n",
    "            break\n",
    "    else:\n",
    "        torch.save(model.state_dict(), '/root/share/model/weight.pt')\n",
    "        trigger_times = 0\n",
    "        best_accuracy = current_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (word_embeddings): Embedding(42000, 200)\n",
       "  (lstm): LSTM(200, 100, batch_first=True)\n",
       "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = LSTM(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "test_model.load_state_dict(torch.load('/root/share/model/weight.pt'))\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_dataloader = DataLoader(test, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.Tensor([])\n",
    "with torch.no_grad():\n",
    "    for data in infer_dataloader:\n",
    "        outputs = test_model(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        res = torch.cat([res, predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.tolist()\n",
    "res = list(map(int, res))\n",
    "submission_data = {'index':[i for i in range(5000)], 'category' : res}\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.to_csv('/root/share/data/lstm_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
