{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gDiJAC3nvp7E"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2054,"status":"ok","timestamp":1666198825065,"user":{"displayName":"고기호","userId":"09981987576142340132"},"user_tz":-540},"id":"lhca5wtLxfOM","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["f8e3731a749743e5984e5e3f5db0f470","1050ff7e5477404fb71bd722f5cbd092","e4c6dfab9c914dcf8de11047f9a9c2f2","a58617629a9a4264b0751eb3855f71ae","59780654811c43a0b18a9c6f184fad72","3287d448a96d4d2389295b4771baaa29","5f1a73ae9a8d42e4890bcf0041b666e6","6c8156fc6e8f46128aa8ba833b91a1aa","cf82e1bf899e470389e394ac187be342","833c050aff0f4e7f9a9723d033422a99","22a15244611b4c3b8523f6ab44ed24e2"]},"outputId":"a80274fd-cad4-4b7c-c804-f3a722259851"},"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"stream","name":"stdout","text":["Moving 0 files to the new cache system\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e3731a749743e5984e5e3f5db0f470"}},"metadata":{}}],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","\n","from transformers import BertTokenizerFast, BertModel\n","from torchtext import transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch\n","from torch.optim import Adam\n","from tqdm import tqdm\n","\n","computer = 'colab'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1666198825065,"user":{"displayName":"고기호","userId":"09981987576142340132"},"user_tz":-540},"id":"pi3YJvWT_0-S"},"outputs":[],"source":["if computer=='docker':\n","    train_data_path = '/root/share/data/train.csv'\n","    test_data_path = '/root/share/data/test.csv'\n","elif computer=='colab':\n","    train_data_path = '/content/drive/MyDrive/commit_folder/chungwadae/torch_nlp/data/train_dataset.pkl'\n","    valid_data_path = '/content/drive/MyDrive/commit_folder/chungwadae/torch_nlp/data/valid_dataset.pkl'\n","    test_data_path = '/content/drive/MyDrive/commit_folder/chungwadae/torch_nlp/data/test_dataset.pkl'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2149,"status":"ok","timestamp":1666198827211,"user":{"displayName":"고기호","userId":"09981987576142340132"},"user_tz":-540},"id":"ybN83s2UC2d7","outputId":"1b62a764-5d8a-4517-fb5e-13cd2bfcd930"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = BertTokenizerFast.from_pretrained('kykim/bert-kor-base')\n","model = BertModel.from_pretrained('kykim/bert-kor-base')"]},{"cell_type":"code","source":["class CustomTrainDataset(Dataset):\n","  def __init__(self, df, tokenizer, tag):\n","    self.labels = [label for label in df['category']]\n","    df['data'] = df['data'].apply(self.tokenizer, tag=tag)\n","    df['data'] = df['data'].apply(lambda x: ' '.join(x))\n","    self.sentence = [\n","        tokenizer(sentence, padding='max_length', max_length=300, truncation=True, return_tensors='pt')\n","        for sentence in df['data']            \n","    ]\n","\n","  def tokenizer(self, sentence, tag):\n","    return tag.morphs(sentence)\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def get_batch_labels(self, idx):\n","    return np.array(self.labels[idx])\n","\n","  def get_batch_sentences(self, idx):\n","    return self.sentence[idx]\n","\n","  def __getitem__(self, idx):\n","\n","    batch_sentence = self.get_batch_sentences(idx)\n","    batch_y = self.get_batch_labels(idx)\n","\n","    return batch_sentence, batch_y\n","\n","class CustomTestDataset(Dataset):\n","  def __init__(self, df, tokenizer, tag):\n","    df['data'] = df['data'].apply(self.tokenizer, tag=tag)\n","    df['data'] = df['data'].apply(lambda x: ' '.join(x))\n","    self.sentence = [\n","        tokenizer(sentence, padding='max_length', max_length=300, truncation=True, return_tensors='pt')\n","        for sentence in df['data']            \n","    ]\n","\n","  def tokenizer(self, sentence, tag):\n","    return tag.morphs(sentence)\n","\n","  def __len__(self):\n","    return len(self.sentence)\n","\n","  def get_batch_sentences(self, idx):\n","    return self.sentence[idx]\n","\n","  def __getitem__(self, idx):\n","\n","    batch_sentence = self.get_batch_sentences(idx)\n","\n","    return batch_sentence"],"metadata":{"id":"lfRIuZ26Pteb","executionInfo":{"status":"ok","timestamp":1666198827211,"user_tz":-540,"elapsed":2,"user":{"displayName":"고기호","userId":"09981987576142340132"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":30129,"status":"ok","timestamp":1666198857338,"user":{"displayName":"고기호","userId":"09981987576142340132"},"user_tz":-540},"id":"KBWIe1Ik0z0-"},"outputs":[],"source":["with open(train_data_path, 'rb') as f:\n","  train_dataset = pickle.load(f)\n","\n","with open(valid_data_path, 'rb') as f:\n","  valid_dataset = pickle.load(f)\n","\n","with open(test_data_path, 'rb') as f:\n","  test_dataset = pickle.load(f)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"DIkALxQH28YJ","executionInfo":{"status":"ok","timestamp":1666198857339,"user_tz":-540,"elapsed":14,"user":{"displayName":"고기호","userId":"09981987576142340132"}}},"outputs":[],"source":["class BertClassifier(nn.Module):\n","  def __init__(self, dropout=0.5):\n","    super(BertClassifier, self).__init__()\n","\n","    self.bert = BertModel.from_pretrained('kykim/bert-kor-base')\n","    self.dropout = nn.Dropout(dropout)\n","    self.linear = nn.Linear(768,3)\n","    self.softmax = nn.Softmax()\n","\n","  def forward(self, input, mask):\n","    _, x = self.bert(input, attention_mask=mask, return_dict=False) # x는 [CLS]의 토큰의 정보를 담고 있음\n","    x = self.dropout(x)\n","    x = self.linear(x)\n","    output = self.softmax(x)\n","    return output"]},{"cell_type":"code","source":["def train(model, train_data, val_data, lr, epochs):\n","  train_dataloader = DataLoader(train_data, batch_size = 16, shuffle=True)\n","  valid_dataloader = DataLoader(val_data, batch_size = 16, shuffle=False)\n","\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device('cuda' if use_cuda else 'cpu')\n","\n","  loss_func = nn.CrossEntropyLoss()\n","  optimizer = Adam(model.parameters(), lr = lr)\n","\n","  if use_cuda:\n","    model = model.cuda()\n","    loss_func = loss_func.cuda()\n","\n","  for epoch in range(epochs):\n","    total_train_accuracy = 0.0\n","    total_train_loss = 0.0\n","\n","    for input, label in tqdm(train_dataloader):\n","      label = label.to(device)\n","      mask = input['attention_mask'].squeeze(1).to(device)\n","      input_id = input['input_ids'].squeeze(1).to(device)\n","\n","      output = model(input_id, mask)\n","\n","      batch_loss = loss_func(output, label)\n","      total_train_loss += batch_loss.item()\n","\n","      accuracy = (output.argmax(dim=1) == label).sum().item()\n","      total_train_accuracy += accuracy\n","\n","      model.zero_grad()\n","      batch_loss.backward()\n","      optimizer.step()\n","\n","    total_val_accuracy = 0.0\n","    total_val_loss = 0.0\n","\n","    with torch.no_grad():\n","      for input, label in valid_dataloader:\n","        label = label.to(device)\n","        mask = input['attention_mask'].squeeze(1).to(device)\n","        input_id = input['input_ids'].squeeze(1).to(device)\n","\n","        output = model(input_id, mask)\n","\n","        batch_loss = loss_func(output, label)\n","        total_val_loss += batch_loss.item()\n","\n","        accuracy = (output.argmax(dim=1) == label).sum().item()\n","        total_val_accuracy += accuracy\n","\n","    print(\n","      f'Epochs: {epoch + 1} | Train Loss: {total_train_loss / len(train_data): .3f} \\\n","      | Train Accuracy: {total_train_accuracy / len(train_data): .3f} \\\n","      | Val Loss: {total_val_loss / len(val_data): .3f} \\\n","      | Val Accuracy: {total_val_accuracy / len(val_data): .3f}')"],"metadata":{"id":"wgbqZq7YQ-DK","executionInfo":{"status":"ok","timestamp":1666198857339,"user_tz":-540,"elapsed":13,"user":{"displayName":"고기호","userId":"09981987576142340132"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 5\n","model = BertClassifier()\n","lr = 1e-6\n","\n","train(model, train_dataset, valid_dataset, lr, EPOCHS)"],"metadata":{"id":"W7v-ahU3U0Lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def infer(model, test_data):\n","  test_dataloader = DataLoader(test_data, batch_size=16, shuffle=False)\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device('cuda' if use_cuda else 'cpu')\n","\n","  if use_cuda:\n","    model = model.cuda()\n","  res = []\n","  for input in tqdm(test_dataloader):\n","    mask = input['attention_mask'].squeeze(1).to(device)\n","    input_id = input['input_ids'].squeeze(1).to(device)\n","    output = model(input_id, mask)\n","\n","    accuracy = output.argmax(dim=1)\n","    res.extend(accuracy.tolist())\n","  return res"],"metadata":{"id":"_u4TbtgnAwQe","executionInfo":{"status":"ok","timestamp":1666240663989,"user_tz":-540,"elapsed":390,"user":{"displayName":"고기호","userId":"09981987576142340132"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["res = infer(model, test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7A8y3kyqkJ1","executionInfo":{"status":"ok","timestamp":1666240768476,"user_tz":-540,"elapsed":103325,"user":{"displayName":"고기호","userId":"09981987576142340132"}},"outputId":"b0d7544f-50ea-476f-d462-1195cfa7030a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/313 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  \n","100%|██████████| 313/313 [01:43<00:00,  3.03it/s]\n"]}]},{"cell_type":"code","source":["submission = pd.read_csv('/content/drive/MyDrive/commit_folder/chungwadae/torch_nlp/data/sample_submission.csv')\n","submission['category'] = res\n","submission.to_csv('/content/drive/MyDrive/commit_folder/chungwadae/torch_nlp/data/bert_submission.csv', index=False)"],"metadata":{"id":"qKXe9c6xqmXG","executionInfo":{"status":"ok","timestamp":1666240852920,"user_tz":-540,"elapsed":308,"user":{"displayName":"고기호","userId":"09981987576142340132"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1XY-DUOdsQmU"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.7.13 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.13"},"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"f8e3731a749743e5984e5e3f5db0f470":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1050ff7e5477404fb71bd722f5cbd092","IPY_MODEL_e4c6dfab9c914dcf8de11047f9a9c2f2","IPY_MODEL_a58617629a9a4264b0751eb3855f71ae"],"layout":"IPY_MODEL_59780654811c43a0b18a9c6f184fad72"}},"1050ff7e5477404fb71bd722f5cbd092":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3287d448a96d4d2389295b4771baaa29","placeholder":"​","style":"IPY_MODEL_5f1a73ae9a8d42e4890bcf0041b666e6","value":""}},"e4c6dfab9c914dcf8de11047f9a9c2f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c8156fc6e8f46128aa8ba833b91a1aa","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf82e1bf899e470389e394ac187be342","value":0}},"a58617629a9a4264b0751eb3855f71ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_833c050aff0f4e7f9a9723d033422a99","placeholder":"​","style":"IPY_MODEL_22a15244611b4c3b8523f6ab44ed24e2","value":" 0/0 [00:00&lt;?, ?it/s]"}},"59780654811c43a0b18a9c6f184fad72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3287d448a96d4d2389295b4771baaa29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f1a73ae9a8d42e4890bcf0041b666e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c8156fc6e8f46128aa8ba833b91a1aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"cf82e1bf899e470389e394ac187be342":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"833c050aff0f4e7f9a9723d033422a99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22a15244611b4c3b8523f6ab44ed24e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}